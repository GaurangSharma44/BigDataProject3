\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{fontenc}
\usepackage{listings}
\usepackage{multicol}
\usepackage{verbatim}

\title{UNamur\\
	ICYBM201 Big Data and Computer Security : Fame for sale, efficient detection of fake Twitter
followers}

\author{TIO NOGUERAS Gérard, NYAKI Loïc}

\begin{document}
\maketitle

\newpage
\tableofcontents
\newpage

\section{Introduction}

\paragraph{}
The objective of this project is to partially reproduce the results presented in a 2015 paper titled  \textit{Fame for sale: effective detection of fake twitter followers}, by Cresci \textit{et al.}\\

In the paper, various classification rules and features proposed by academics, \textit{technology bloggers} and companies specialized in fake twitter users detection are enumerated and assessed. Rule-based classification is shown to perform more poorly than feature-based classification. Therefore the authors decided to abandon rule-based classification in favor of the 19 features responsible for the most information gain while having the smallest calculation cost.\\

The resulting classifier trained on that 19-features set shows an accuracy of up to 97.5\% on randomly sampled account, while being solely based on data readily available on the profile of the twitter users, which ensures that the data processing is fast and lightweight.

\subsection{Objectives}
In this project, we focus on reproducing the final results of Cresci \textit{et al.}, by implementing various classifiers based on the feature set presented in that paper.\\

First we describe the 19 features that will be part of the feature set. Then, we introduce the datasets that have been provided, and which contain \textit{human} or \textit{bot} data. Later we describe how the features will be extracted from these datasets as well as our planned procedure for training classifier. Each type of classifier is then described along with their parameters, and results are presented for each classifier. Finally, we summarize the results argue on the difference between the results presented in Cresci \textit{et al.}, and the results we managed to obtain.


\section{Rule sets and Features set}
In the paper, five rule sets and features set are analyzed. They come from  academic research, as well as technology bloggers and companies specialized in fake tweeter users detection.\\

The rule set contained the following classes of rules:

\begin{itemize}
\item Camisani-Calzolari: The Camisani-Calzolari rule set is described as follows: 22 class A rules
\item State of search:They propose 7 rules, with 5 of class A, and 2 of class B
\item SocialBakers: The rule set of social baker is composed of 9 rules, with 6 of class A and 3 of class B.
\item Stringhini et al.: This paper proposes 5 features with 3 class A features, and 2 class B features
\item Yang et al.:This paper proposes 9 features. Two of class A, 3 of class B, and 4 of class C.
\end{itemize}

\begin{comment}
\subsection{State of search}


\subsection{Socialbakers}

\subsection{Stringhini et al.}

\begin{multicols}{2}
\begin{enumerate}
\item the profile contains a name;
\item the profile contains an image;
\item the profile contains a biography
\item the account has at least 30 followers;
\item it has been inserted in a  list by other Twitter users;
\item it has written at least 50 tweets;
\item the account has been geo-localized;
\item the profile contains a URL;
\item it has been included in another user's favorites;
\item it writes tweets that have punctuation;
\item it has used a hashtag in at least one tweet;
\item it has logged into Twitter using an iPhone;
\item it has logged into Twitter using an Android device;
\item it is connected with Foursquare;
\item it is connected with Instagram;
\item it has logged into the twitter.com website
\item it has written the userID of another user in at least one tweet (that is, it posted a @reply or a mention);
\item (2*number of followers) $\geq$ (number of friends);
\item it publishes content which does not just contain URLs;
\item at least one of its tweets has been retwitted by other accounts (worth 2 points)
\item it has logged into Twitter through different clients (worth 3 points).
\end{enumerate}
\end{multicols}


\subsection{State of search}


\subsection{Socialbakers}

\subsection{Stringhini et al.}

\begin{tabular}{cc}
\hline
1. spambots do not have thousands of friends; & spambots have a high ratio of tweets containing urls\\
2. spambots have sent less than 20 tweets; & spambots have a high ratio between the total number of tweets from friends and the square of their total followers (lower ratio means legitimate user)\\
3. the content of the spambots' tweets exhibits "message similarity"; & \\
\hline
\end{tabular}


\subsection{Yang et al.}
\end{comment}

\subsection{Rules that were not implemented}
The following features where not implemented,as knowing whether a tweet came from an API was far from obvious : \\

\begin{itemize}
\item{get\_api\_url\_ratio(): returns the ratio between the number of tweets containing a URL and the number of tweets sent from an API.}
\item{get\_api\_tweet\_similarity(): supposedly returns a value representing the similarity between tweets sent from an API.}
\end{itemize} 

We tried using the \textit{grep} command with the keyword "API" on the tweets.csv files but only detected a few results who seemed unrelated.

\section{Data extraction}
To generate the final feature set, we extracted the class A features (data easily obtained through the user profile) as well as the class C features (all the features, ranging from the easiest to obtain to the hardest one, requiring many computations).\\

The result was a features dataset that is ready to be used for training classifiers.

\subsection{Available data}
The researchers responsible for the paper Fame forl sale made their 5 basic datasets available for future researchers.\\ 

Since we are trying to replicate the work they have done to practice our skills and understand more deeply the meaning of data and the proper way to analyse data

\subsection{Table 1 creation}
In this section we are going to create the base dataset that we will use throughout the project.
We have 5 available datasets: 
We are going to create 1 final dataset. The BAS dataset constituted of 1950 human twitter accounts and 1950 fake accounts.\\ The human accounts are simply the sum of the human datasets we had available.\\
For the fake accounts, we randomly undersampled the 3 datasets available to obtain the same number of accounts as the normal ones.\\
After undersampling the users, we used the ids of these users to collect the rest of the data in the other files.\\\\
Small differences due to the randomness of the users choice:
We encountered a significant drop first with the number of tweets averaging 92k followers which far from the 118k from the paper. We later realized that our parser had and issue with the tweets containing commas and was missing them because of that error.\\

\begin{tabular}{lcccccc}
dataset & accounts & tweets & followers & friends & total relationships \\
\hline
TFP & 469 & 563,693 & 258,494 & 241,710 & 500,204 \\
E13 & 1481 & 2,068,037 & 1,526,944 & 667,225 & 2,194,169 \\
FSF & 1169 & 22,910 & 11,893 & 253,026 & 264,919 \\
INT & 1337 & 58925 & 23173 & 517485 & 540658 \\
TWT & 845 & 114192 & 28588 & 729839 & 758427 \\
\hline
HUM & 1950 & 2,631,730 & 1,785,438 & 908,935 & 2,694,373 \\
FAK & 1950 & 107,031 & 35,404 & 873,494 & 908,898 \\
\hline
BAS & 3900 & 2,738,761 & 1,820,842 & 1,782,429 & 3,603,271 \\
\end{tabular}


\section{Data Pre-Processing}
\paragraph{}
The datasets that we received was composed of several directories containing each the following files: \textit{users.csv, friends.csv, followers.csv and tweets.csv}. These are regular files containing text, numerical values and NaN values.\\

NaN values are bothersome as they are there own type and can cause problems when they get mixed with Strings or numerals. Therefore the first thing to do was to replace every NaN instance by something less troublesome. We decided that an empty String would be a good solution. This was done in a single command, when opening the CSV file:

\begin{lstlisting}
pd.read_csv(totalPath, encoding='latin-1').fillna('')
\end{lstlisting}

\section{Feature set generation}
Based on \textit{Cresci et al.}, 3 classes of features where identified: classes\textit{A, B} and \textit{C}. Class \textit{A} features being features whose data can be obtained directly in the user profile, while class \textit{B} features require a simple computation, and \textit{C} features require heavier computations.\\

We implemented the functions and used them to extract 2 features set out of the data: a \textit{class A} features set and a class C features set, which encompasses the features of every classes (\textit{A, B} and \textit{C}).

\subsection{Data labeling}
For each initial dataset (E13, FAK, FSF, HUM, INT, TFP, TWT) the label of the users is known. The TFP and HUM dataset only contain real users, while the other dataset contain fake users.\\

Therefor, when generating the class A and class C features set, we add the label 'human' for the features set based on TFP and HUM, and the label 'bot' to the other. These labels will actually be numbers in the features set : 1 for bots, and 0 for humans.

\section{Process Optimization}
The amount of data that we had to manipulate by no means huge or overwhelming, but it was still big enough so that, if not careful, some optimization issues could arise.\\

Developing using Python, we used the popular pandas library and its DataFrame object to manipulate our data and perform computations. Soon however, the program was plagued by an abnormally slow execution time.\\

The issue came from manually looping on DataFrame object, either by user a \textit{for} loop, or by using the DataFrame.iterrows() function. This led to processing time that could reach between 10 and 20 seconds per user record. Considering that we had several thousands users, another solution had to be found.

\subsection{DataFrame.apply() and lambda functions}
We found out that there was several efficient ways to loop over a DataFrame. The easiest way being through the use of lambda functions which will automatically be executed on every row of the Dataframe, without requiring a manual management of the iterative process.\\

The result was efficient, and the processing time per user came down from 10-20s to about 0.3s. We decided that this gain in performance was enough, and didn't try to improve further on it, even though we heard of a method called Vectorization which would arguably perform even faster.

\section{Classifiers}
In \textit{Cresci et al.} the following 8 classifiers are proposed: Random Forest, Decorate, Decision Tree, Adaptive Boosting, Bayesian Network, k-Nearest Neighbors, Logistic Regression and Support Vector Machine.\\

For the sake of simplicity, we decided to focus on the following classifiers:Random Forest,  Decision Tree, Adaptive Boosting, k-Nearest Neighbors, Logistic Regression and Support Vector Machine.\\

\subsection{Results}
After training our classifiers, we obtain the following resutls:

\begin{tabular}{lcccccc}
	\hline
	\textbf{Algorithm} 	& \textbf{Accuracy} 	& \textbf{Precision} & \textbf{Recall} &\textbf{F-M} 	& \textbf{MCC} & AUC\\
	\hline
	\textit{Class A}\\
	SVM	&	0.682	&	0.999	&	0.364	& 0.533 & 0.47 & 0.68\\
	AB	&	1		&	1		&	1		& 1 	&	1	& 1   \\
	RF	&	1		&	1		&	1		& 1   	&	1	& 1  \\
	kNN	&	0.947	&	0.94	&	0.956	& 0.948 & 0.895 & 0.947\\
	J48	&	1		&	1		&	1		& 1 	&	1	& 1   \\
	LR	&	1		&	1		&	1		& 1 	&	1	& 1   \\
	\hline
	\textbf{Algorithm} 	& \textbf{Accuracy} 	& \textbf{Precision} & \textbf{Recall} & \textbf{F-M} 	& \textbf{MCC} & \textbf{AUC}\\
	\hline
	SVM	&	0.503	&	1		&	0.006	& 0.012 &  0.056   & 0.503 \\
	AB	&	1		&	1		&	1		& 1 	&	1		& 1   \\
	RF	&	1		&	1		&	0.999	& 1   	&	0.999	& 1  	\\
	kNN	&	0.892	&	0.853	&	0.949	& 0.898 &   0.79 	& 0.892 \\
	J48	&	1		&	1		&	1		& 1 	&	1		& 1   	\\
	LR	&	0.999	&	0.998	&	0.999	& 0.999 &	0.998	& 0.999   \\
		
\end{tabular}
	
	Due to an execution error caused by a bad value, the results for class C couldn't be displayed
	
\section{Conclusion}
In this project, we first familiarized ourselves with the various dataset provided before starting building features that emulated the 5 studies described in Cresci \textit{et al}. \\
Based on those features and and the Class A, and Class C feature set provided by Cresci \textit{et al.}, we generated for each of the 6 datasets, a pair of Class A and Class C features dataset for which every row was labeled as either 'human' or 'bot'.\\
Once these features were generated and labeled, they were used them to train various \textit{classifiers} in hope to be able to distinguish fake users from legitimate users.

\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}