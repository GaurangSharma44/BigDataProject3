\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\definecolor{lightgray}{rgb}{0.9,0.9,0.9}
\definecolor{darkgray}{rgb}{0.4,0.4,0.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\usepackage{url}
\usepackage[top=3cm,bottom=3cm,left=3cm,right=3cm]{geometry}

\title{UNamur\\
	ICYBM201 Big Data and Computer Security : Fame for sale, efficient detection of fake Twitter
followers}

\author{TIO NOGUERAS Gérard, NYAKI Loïc}

\begin{document}
\maketitle

\newpage
\tableofcontents
\newpage

\section{Introduction}
\paragraph{}
The objective of this project is to partially reproduce the results presented in a 2015 paper titled  \textit{Fame for sale: effective detection of fake twitter followers}, by Cresci \textit{et al.}\\

In the paper various classification rules and features proposed by academics, \textit{technology bloggers} and companies specialized in fake twitter users detection are enumerated and assessed. Rule-based classification is shown to be perform more poorly than feature-based classification. Therefore, rule-based classification is dropped, and the 19 features responsible for the most information gain, and the smallest calculation cost are selected as the \textit{feature set}.\\

The resulting classifier shows an accuracy of up to 97.5\% on randomly sampled account, while being solely based on data readily available on the profile of the twitter users.

\subsection{Objectives}
In this project, we focus on reproducing the final results of Cresci \textit{et al.}, by implementing various classifiers based on the feature set presented in that paper.\\

First we describe the 19 features that will be part of the feature set. Then, we introduce the datasets on which the classifiers will be trained. Later we describe how the features will be extracted from these datasets as well as our planned procedure for training classifier. Each type of classifier is then described along with their parameters, and results are presented for each classifier. Finally, we summarize the results argue on the difference between the results presented in Cresci \textit{et al.}, and the results we managed to obtain.


\section{Features set}
TODO:
Je choisi de présenter les features avant les données, afin qu'une fois les données brutes présentées, on puisse enchainer avec l'extraction et pre-processing.

Comme on connait déjà les features, ça marche assez bien si on fait les choses dans cet ordre là, je trouve.

\section{Data extraction}

\subsection{Available data}
The researchers responsible for the paper Fame for sale, made their 5 basic datasets available for future researchers.\\ Since we are trying to replicate the work they have done to practice our skills and understand more deeply the meaning of data and the proper way to analyse data
\subsection{Table 1 creation}
In this section we are going to create the base dataset that we will use throughout the project.
We have 5 available datasets: 
We are going to create 1 final dataset. The BAS dataset constituted of 1950 human twitter accounts and 1950 fake accounts.\\ The human accounts are simply the sum of the human datasets we had available.\\
For the fake accounts, we randomly undersampled the 3 datasets available to obtain the same number of accounts as the normal ones.\\
After undersampling the users, we used the ids of these users to collect the rest of the data in the other files.\\\\
Small differences due to the randomness of the users choice:
We encountered a significant drop first with the number of tweets averaging 92k followers which far from the 118k from the paper. We later realized that our parser had and issue with the tweets containing commas and was missing them because of that error.\\

\begin{tabular}{lcccccc}
dataset & accounts & tweets & followers & friends & total relationships \\
\hline
TFP & 469 & 563,693 & 258,494 & 241,710 & 500,204 \\
E13 & 1481 & 2,068,037 & 1,526,944 & 667,225 & 2,194,169 \\
FSF & 1169 & 22,910 & 11,893 & 253,026 & 264,919 \\
INT & 1337 & 58925 & 23173 & 517485 & 540658 \\
TWT & 845 & 114192 & 28588 & 729839 & 758427 \\
\hline
HUM & 1950 & 2,631,730 & 1,785,438 & 908,935 & 2,694,373 \\
FAK & 1950 & 107,031 & 35,404 & 873,494 & 908,898 \\
\hline
BAS & 3900 & 2,738,761 & 1,820,842 & 1,782,429 & 3,603,271 \\
\end{tabular}


\section{Data Pre-Processing}



\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}